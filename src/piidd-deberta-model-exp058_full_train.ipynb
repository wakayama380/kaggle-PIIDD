{"cells":[{"cell_type":"markdown","id":"78bdf3a9","metadata":{"id":"78bdf3a9"},"source":["- This one is based on the great starting [notebook](https://www.kaggle.com/code/nbroad/transformer-ner-baseline-lb-0-576) by @NICHOLAS BROAD.\n","- please note: I didn't use the deberta-base model I choose here to finetune, this is just a demo.\n","- In the postprocessing part of inference, I changed **tokens** to **token_map** in two lines. This will bring a huge improvement. There is the [inference notebook](https://www.kaggle.com/code/takanashihumbert/piidd-deberta-model-starter-inference)"]},{"cell_type":"code","execution_count":null,"id":"C-EH2UjG0zxK","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1713522923370,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"C-EH2UjG0zxK","outputId":"758f800c-1a91-4a35-94eb-283483bd92d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fri Apr 19 10:35:23 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0              43W / 400W |      2MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"id":"b360F4bv02AV","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24587,"status":"ok","timestamp":1713522947948,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"b360F4bv02AV","outputId":"babbd10e-ef47-4a41-d8ae-11340ec82e30"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"SehFB6CJ02op","metadata":{"id":"SehFB6CJ02op"},"outputs":[],"source":["from google.colab import runtime"]},{"cell_type":"code","execution_count":null,"id":"9nVb936Y0-Bx","metadata":{"id":"9nVb936Y0-Bx"},"outputs":[],"source":["class CFG:\n","  exp='exp058_full_train'\n","  max_len = 1024\n","  stride = 256\n","  model_name = 'microsoft/deberta-v3-large'\n","  seed=42\n","  target = [\n","    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM',\n","    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM',\n","    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n","  ]"]},{"cell_type":"code","execution_count":null,"id":"Oam9kePE1D5p","metadata":{"id":"Oam9kePE1D5p"},"outputs":[],"source":["# ====================================================\n","# Directory settings\n","# ====================================================\n","import os\n","\n","OUTPUT_DIR = f'/content/drive/MyDrive/Kaggle/competition/PIIDD/outputs/{CFG.exp}/'\n","\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)"]},{"cell_type":"code","execution_count":null,"id":"6bedb4f7","metadata":{"id":"6bedb4f7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"367597d9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":97084,"status":"ok","timestamp":1713523046585,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"367597d9","outputId":"201ea215-f2cd-4f0c-f4e1-44f710a7bff5"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Collecting transformers==4.31.0\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.22.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2024.2.2)\n","Installing collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.15.2\n","    Uninstalling tokenizers-0.15.2:\n","      Successfully uninstalled tokenizers-0.15.2\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.38.2\n","    Uninstalling transformers-4.38.2:\n","      Successfully uninstalled transformers-4.38.2\n","Successfully installed tokenizers-0.13.3 transformers-4.31.0\n","Requirement already satisfied: tokenizers==0.13.3 in /usr/local/lib/python3.10/dist-packages (0.13.3)\n","Collecting accelerate\n","  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.22.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n","Successfully installed accelerate-0.29.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"]}],"source":["!pip install seqeval evaluate -q\n","!pip install sentencepiece\n","\n","!pip install transformers==4.31.0\n","os.system('pip install -q tokenizers')\n","!pip install tokenizers==0.13.3\n","\n","!pip install accelerate -U\n","\n"]},{"cell_type":"code","execution_count":null,"id":"351f8f01","metadata":{"id":"351f8f01"},"outputs":[],"source":["import json\n","import argparse\n","from itertools import chain\n","from functools import partial\n","\n","import torch\n","from torch.nn import CrossEntropyLoss\n","from transformers import AutoTokenizer, Trainer, TrainingArguments\n","from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\n","import evaluate\n","from datasets import Dataset, features, DatasetDict\n","# import datasets\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"id":"QbxuMv2Sp-Lh","metadata":{"id":"QbxuMv2Sp-Lh"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"id":"8pjiA436NXoZ","metadata":{"id":"8pjiA436NXoZ"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"id":"a20e7f96","metadata":{"id":"a20e7f96"},"outputs":[],"source":["import json\n","input_path = '/content/drive/MyDrive/Kaggle/competition/PIIDD/input/'\n","data = json.load(open(input_path+\"train.json\"))\n","\n","print(len(data))\n","print(data[0].keys())"]},{"cell_type":"code","execution_count":null,"id":"-UdkoaeDp61X","metadata":{"id":"-UdkoaeDp61X"},"outputs":[],"source":["# input_path = '/content/drive/MyDrive/Kaggle/competition/PIIDD/input/'\n","# external_data = pd.read_csv(input_path+'pii_dataset.csv')\n","\n","# external_data=external_data[['text','document','tokens','trailing_whitespace','labels']].rename(columns={'text':'full_text','labels':'provided_labels'})\n","\n","# ds_external = Dataset.from_pandas(external_data)\n"]},{"cell_type":"code","execution_count":null,"id":"4f22No0Iqm-E","metadata":{"id":"4f22No0Iqm-E"},"outputs":[],"source":["import json\n","input_path = '/content/drive/MyDrive/Kaggle/competition/PIIDD/input/'\n","\n","external_data = json.load(open(input_path+\"mixtral-8x7b-v1.json\"))\n","# external_data1 = json.load(open(input_path+\"moredata_dataset_fixed.json\"))\n","# external_data2 = json.load(open(input_path+\"mpware_mixtral8x7b_v1.1-no-i-username.json\"))\n","\n","# external_data=external_data+external_data1+external_data2\n","\n","print(len(external_data))\n","print(external_data[0].keys())"]},{"cell_type":"code","execution_count":null,"id":"eq7bhcOiGyzV","metadata":{"id":"eq7bhcOiGyzV"},"outputs":[],"source":["train_data=data+external_data\n","\n","for i in range(len(train_data)):\n","  train_data[i]['document']=i+1"]},{"cell_type":"code","execution_count":null,"id":"9LdBdZ75Noiv","metadata":{"colab":{"background_save":true},"id":"9LdBdZ75Noiv"},"outputs":[],"source":["for i in range(len(train_data)):\n","  for idx,labels in enumerate(train_data[i]['labels']):\n","    if labels in CFG.target:\n","      train_data[i]['labels'][idx]=train_data[i]['labels'][idx][2:]"]},{"cell_type":"code","execution_count":null,"id":"cd588b57","metadata":{"colab":{"background_save":true},"id":"cd588b57","outputId":"ccbd338b-93e3-4c92-9107-d0375a824481"},"outputs":[{"name":"stdout","output_type":"stream","text":["{0: 'EMAIL', 1: 'ID_NUM', 2: 'NAME_STUDENT', 3: 'PHONE_NUM', 4: 'STREET_ADDRESS', 5: 'URL_PERSONAL', 6: 'USERNAME', 7: 'O'}\n"]}],"source":["all_labels = [\n","    'EMAIL', 'ID_NUM', 'NAME_STUDENT', 'PHONE_NUM',\n","    'STREET_ADDRESS', 'URL_PERSONAL', 'USERNAME','O'\n","  ]\n","label2id = {l: i for i,l in enumerate(all_labels)}\n","id2label = {v:k for k,v in label2id.items()}\n","\n","print(id2label)"]},{"cell_type":"code","execution_count":null,"id":"02a79668","metadata":{"colab":{"background_save":true},"id":"02a79668"},"outputs":[],"source":["target = [\n","    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM',\n","    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM',\n","    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n","]"]},{"cell_type":"code","execution_count":null,"id":"Ra07Ha84hZy3","metadata":{"colab":{"background_save":true},"id":"Ra07Ha84hZy3"},"outputs":[],"source":["def is_nested_list(lst):\n","    return any(isinstance(item, list) for item in lst)"]},{"cell_type":"code","execution_count":null,"id":"18e6407d","metadata":{"colab":{"background_save":true},"id":"18e6407d"},"outputs":[],"source":["def tokenize(example, tokenizer, label2id):\n","    text = []\n","\n","    # these are at the character level\n","    labels = []\n","    targets = []\n","\n","    for t, l, ws in zip(example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]):\n","        text.append(t)\n","        #文字レベルでlabelを振っている．\n","        labels.extend([l]*len(t))\n","\n","        if l in CFG.target:\n","            targets.append(1)\n","        else:\n","            targets.append(0)\n","        # if there is trailing whitespace\n","        if ws:\n","            text.append(\" \")\n","            labels.append(\"O\")\n","\n","    target_num = sum(targets)\n","    labels = np.array(labels)\n","\n","    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=CFG.max_len, stride=CFG.stride,return_overflowing_tokens=True)\n","    text = \"\".join(text)\n","\n","    token_samples=[]\n","\n","    if not is_nested_list(tokenized.offset_mapping):\n","      token_labels = []\n","      for start_idx, end_idx in tokenized.offset_mapping:\n","\n","          # CLS token\n","          if start_idx == 0 and end_idx == 0:\n","              token_labels.append(label2id[\"O\"])\n","              continue\n","\n","          # case when token starts with whitespace\n","          if text[start_idx].isspace() and text[start_idx]!='\\n' and text[start_idx]!='\\n\\n':\n","              start_idx += 1\n","\n","          token_labels.append(label2id[labels[start_idx]])\n","\n","      length = len(tokenized.input_ids)\n","\n","      return [\n","          tokenized.input_ids,\n","          tokenized.token_type_ids,\n","          tokenized.attention_mask,\n","          tokenized.offset_mapping,\n","          tokenized.overflow_to_sample_mapping,\n","          token_labels,\n","          length,\n","          target_num,\n","          1 if target_num>0 else 0,\n","          example['document'],\n","      ]\n","\n","    else:\n","      for i in range(len(tokenized.offset_mapping)):\n","        token_labels = []\n","\n","        for start_idx, end_idx in tokenized.offset_mapping[i]:\n","\n","            # CLS token\n","            if start_idx == 0 and end_idx == 0:\n","                token_labels.append(label2id[\"O\"])\n","                continue\n","\n","            # case when token start\n","            if text[start_idx].isspace() and text[start_idx]!='\\n' and text[start_idx]!='\\n\\n':\n","                start_idx += 1\n","\n","            token_labels.append(label2id[labels[start_idx]])\n","\n","        length = len(tokenized.input_ids[i])\n","\n","        token_sample= [\n","          tokenized.input_ids[i],\n","          tokenized.token_type_ids[i],\n","          tokenized.attention_mask[i],\n","          tokenized.offset_mapping[i],\n","          tokenized.overflow_to_sample_mapping[i],\n","          token_labels,\n","          length,\n","          target_num,\n","          1 if target_num>0 else 0,\n","          example['document'],\n","      ]\n","        token_samples.append(token_sample)\n","\n","      return token_samples"]},{"cell_type":"code","execution_count":null,"id":"573fe1ed","metadata":{"id":"573fe1ed"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n","tokenizer.add_tokens(['\\n','\\n\\n'], special_tokens=True)\n","\n","train_ds = Dataset.from_dict({\n","    \"full_text\": [x[\"full_text\"] for x in train_data],\n","    \"document\": [x[\"document\"] for x in train_data],\n","    \"tokens\": [x[\"tokens\"] for x in train_data],\n","    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in train_data],\n","    \"provided_labels\": [x[\"labels\"] for x in train_data],\n","})"]},{"cell_type":"code","execution_count":null,"id":"WA43yNzM8L5g","metadata":{"id":"WA43yNzM8L5g"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"fd568359","metadata":{"id":"fd568359"},"outputs":[],"source":["%%time\n","data_list=[]\n","for example in train_ds:\n","    # 各サンプルに対してtokenize関数を呼び出し\n","    dataset_list=tokenize(example, tokenizer=tokenizer, label2id=label2id)\n","    for d in dataset_list:\n","      data_list.append(d)\n","\n","\n","train_ds=Dataset.from_dict({\n","  'input_ids': [x[0] for x in data_list],\n","  'token_type_ids': [x[1] for x in data_list],\n","  \"attention_mask\":[x[2] for x in data_list],\n","  'offset_mapping': [x[3] for x in data_list],\n","  'overflow_to_sample_mapping':[x[4] for x in data_list],\n","  \"labels\": [x[5] for x in data_list],\n","  \"length\": [x[6] for x in data_list],\n","  \"target_num\": [x[7] for x in data_list],\n","  \"group\": [x[8] for x in data_list],\n","  'document':[x[9] for x in data_list],\n","  })\n","\n","# ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id}, num_proc=2)\n","\n","train_ds = train_ds.class_encode_column(\"group\")\n"]},{"cell_type":"code","execution_count":null,"id":"vmmsHkGKJ6sv","metadata":{"id":"vmmsHkGKJ6sv"},"outputs":[],"source":["for i in range(32,10000):\n","  if train_ds[i]['group']>0:\n","    break"]},{"cell_type":"code","execution_count":null,"id":"J_Bg6PY0jO1S","metadata":{"id":"J_Bg6PY0jO1S"},"outputs":[],"source":["from transformers import DebertaV2PreTrainedModel, DebertaV2Model\n","from transformers.modeling_outputs import TokenClassifierOutput\n","import torch.nn as nn\n","class LSTMHead(nn.Module):\n","    def __init__(self, in_features, hidden_dim, n_layers):\n","        super().__init__()\n","        self.lstm = nn.LSTM(in_features,\n","                            hidden_dim,\n","                            n_layers,\n","                            batch_first=True,\n","                            bidirectional=True,\n","                            dropout=0.1)\n","        self.out_features = hidden_dim\n","\n","    def forward(self, x):\n","        self.lstm.flatten_parameters()\n","        hidden, (_, _) = self.lstm(x)\n","        out = hidden\n","        return out\n","\n","\n","# Copied from transformers.models.deberta.modeling_deberta.DebertaForTokenClassification with Deberta->DebertaV2\n","class CustomModel(DebertaV2PreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.deberta = DebertaV2Model(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        self.head = LSTMHead(in_features=config.hidden_size, hidden_dim=config.hidden_size//2, n_layers=1)\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n","\n","        outputs = self.deberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","        )\n","\n","        sequence_output = outputs.last_hidden_state\n","        sequence_output = self.dropout(sequence_output)\n","        sequence_output = self.head(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        return TokenClassifierOutput(\n","            loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions\n","        )\n","\n","class CustomTrainer(Trainer):\n","    def __init__(self, *args, class_weights=None, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        # Assuming class_weights is a Tensor of weights for each class\n","        self.class_weights = class_weights\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        # Extract labels\n","        labels = inputs.pop(\"labels\")\n","        # Forward pass\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        # Reshape for loss calculation\n","        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            loss = self.label_smoother(outputs, inputs)\n","        else:\n","            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"markdown","id":"73e07a6f","metadata":{"id":"73e07a6f"},"source":["## competition metrics"]},{"cell_type":"code","execution_count":null,"id":"99cb0e1e","metadata":{"id":"99cb0e1e"},"outputs":[],"source":["model = CustomModel.from_pretrained(\n","    CFG.model_name,\n","    num_labels=len(all_labels),\n","    id2label=id2label,\n","    label2id=label2id,\n","    ignore_mismatched_sizes=True\n",")\n","collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)"]},{"cell_type":"code","execution_count":null,"id":"6b6dd354","metadata":{"id":"6b6dd354"},"outputs":[],"source":["# FREEZE_EMBEDDINGS = False\n","# FREEZE_LAYERS = 6\n","\n","# if FREEZE_EMBEDDINGS:\n","#     print('Freezing embeddings.')\n","#     for param in model.deberta.embeddings.parameters():\n","#         param.requires_grad = False\n","\n","# if FREEZE_LAYERS>0:\n","#     print(f'Freezing {FREEZE_LAYERS} layers.')\n","#     for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n","#         for param in layer.parameters():\n","#             param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"id":"394628c3","metadata":{"id":"394628c3"},"outputs":[],"source":["class CustomTrainer(Trainer):\n","    def __init__(self, *args, class_weights=None, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        # Assuming class_weights is a Tensor of weights for each class\n","        self.class_weights = class_weights\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        # Extract labels\n","        labels = inputs.pop(\"labels\")\n","        # Forward pass\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        # Reshape for loss calculation\n","        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            loss = self.label_smoother(outputs, inputs)\n","        else:\n","            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"markdown","id":"91afd858","metadata":{"id":"91afd858"},"source":["## training"]},{"cell_type":"code","execution_count":null,"id":"9b6c113c","metadata":{"id":"9b6c113c"},"outputs":[],"source":["\n","args = TrainingArguments(\n","    output_dir=OUTPUT_DIR,\n","    fp16=True,\n","    learning_rate=2e-5,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=6,\n","    report_to=\"none\",\n","    evaluation_strategy=\"no\",\n","    do_eval=False,\n","    save_total_limit=1,\n","    logging_steps=100,\n","    lr_scheduler_type='cosine',\n","    warmup_ratio=0.1,\n","    weight_decay=0.01\n",")\n","\n","class_weights=torch.tensor([5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0 ,1.0]).to(device)\n","\n","# Initialize Trainer with custom class weights\n","trainer = CustomTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_ds,\n","    data_collator=collator,\n","    tokenizer=tokenizer,\n","    class_weights=class_weights,\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"nVyeo0Nyi4_1","metadata":{"id":"nVyeo0Nyi4_1"},"outputs":[],"source":["len(train_ds)"]},{"cell_type":"code","execution_count":null,"id":"e908f9fc","metadata":{"id":"e908f9fc","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"9cc00150-1db7-47a7-fff2-4acbd64dd819"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3743' max='5346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3743/5346 45:00 < 19:17, 1.39 it/s, Epoch 2.10/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.351200</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.179500</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.063600</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.022900</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.010700</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.010400</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.005300</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.003400</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.016600</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.006500</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.004000</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.004200</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.003000</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.004600</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.003600</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.005200</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.005500</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.002800</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.004600</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.002500</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.001200</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.003200</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.001000</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.001600</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.003900</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.002600</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.002400</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.000800</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.001200</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.001800</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>0.002600</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.001500</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>0.000600</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.000800</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.002000</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.000700</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>0.000700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3871' max='5346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3871/5346 46:29 < 17:43, 1.39 it/s, Epoch 2.17/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.351200</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.179500</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.063600</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.022900</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.010700</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.010400</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.005300</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.003400</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.016600</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.006500</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.004000</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.004200</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.003000</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.004600</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.003600</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.005200</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.005500</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.002800</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.004600</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.002500</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.001200</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.003200</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.001000</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.001600</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.003900</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.002600</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.002400</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.000800</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.001200</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.001800</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>0.002600</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.001500</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>0.000600</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.000800</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.002000</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.000700</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>0.000700</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}],"source":["%%time\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"id":"o9VKUiWqLNft","metadata":{"id":"o9VKUiWqLNft"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"a9a43f31","metadata":{"id":"a9a43f31"},"outputs":[],"source":["trainer.save_model(OUTPUT_DIR)\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"id":"0120927c","metadata":{"id":"0120927c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"vwVyhQRA-FwF","metadata":{"id":"vwVyhQRA-FwF"},"outputs":[],"source":["runtime.unassign()"]},{"cell_type":"code","execution_count":null,"id":"iqdLFtbz-HOr","metadata":{"id":"iqdLFtbz-HOr"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"datasetId":2210196,"sourceId":3693646,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":1112.91731,"end_time":"2024-01-25T21:34:37.729728","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-25T21:16:04.812418","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}